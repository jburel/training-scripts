# -----------------------------------------------------------------------------
#  Copyright (C) 2018 University of Dundee. All rights reserved.
#
#
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
#  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
# ------------------------------------------------------------------------------

# This Jython script uses ImageJ to analyse particles, and saved the results
# locally in a CSV file. The file is then uploaded and attached to the specified
# project as a file annotation.
# Use this script in the Scripting Dialog of Fiji (File > New > Script).
# Select Python as language in the Scripting Dialog.
# Error handling is omitted to ease the reading of the script but
# this should be added
# if used in production to make sure the services are closed
# Information can be found at
# https://docs.openmicroscopy.org/latest/omero/developers/Java.html


from java.lang import Double
from java.lang import Long
from java.lang import Object
from java.lang import String
from java.util import ArrayList

# OMERO Dependencies
from omero.gateway import Gateway
from omero.gateway import LoginCredentials
from omero.gateway import SecurityContext
from omero.gateway.facility import BrowseFacility, DataManagerFacility, MetadataFacility
from omero.gateway.facility import ROIFacility
from omero.gateway.facility import TablesFacility
from omero.gateway.model import FileAnnotationData
from omero.gateway.model import MapAnnotationData
from omero.gateway.model import DatasetData, ImageData, ProjectData
from omero.gateway.model import TableData, TableDataColumn


from omero.model import DatasetI, ImageI, ProjectI
from omero.log import SimpleLogger
from omero.rtypes import rlong, rstring
from omero.model import ChecksumAlgorithmI, FileAnnotationI, OriginalFileI
from omero.model.enums import ChecksumAlgorithmSHA1160

from org.openmicroscopy.shoola.util.roi.io import ROIReader

from ij import IJ
from ij.plugin.frame import RoiManager
from ij.measure import ResultsTable

# Import required to save the results as CSV
import csv
import os
import fnmatch
import tempfile

# Setup
# =====

# OMERO Server details
HOST = "outreach.openmicroscopy.org"
PORT = 4064
project_id = 1101
USERNAME = "username"
PASSWORD = "password"

NAMESPACE = "openmicroscopy.org/omero/bulk_annotations"
MAP_KEY = "Channels"

save_data = True

if HOST.startswith("idr"):
    save_data = False


def connect_to_omero():
    "Connect to OMERO. Returns a reference to the gateway"

    credentials = LoginCredentials()
    credentials.getServer().setHostname(HOST)
    credentials.getServer().setPort(PORT)
    credentials.getUser().setUsername(USERNAME.strip())
    credentials.getUser().setPassword(PASSWORD.strip())
    simpleLogger = SimpleLogger()
    gateway = Gateway(simpleLogger)

    user = gateway.connect(credentials)
    print user.getGroupId()
    return gateway, user


def get_datasets(gateway, ctx, project_id):
    "List all dataset's ids contained in a Project"

    browse = gateway.getFacility(BrowseFacility)
    ids = ArrayList(1)
    val = Long(project_id)
    ids.add(val)
    projects = browse.getProjects(ctx, ids)
    return projects[0].getDatasets()


def get_image_ids(gateway, ctx, dataset_id):
    "List all image's ids contained in a Dataset"

    browse = gateway.getFacility(BrowseFacility)

    ids = ArrayList(1)
    val = Long(dataset_id)
    ids.add(val)
    images = browse.getImagesForDatasets(ctx, ids)

    j = images.iterator()
    image_ids = []
    while j.hasNext():
        image = j.next()
        image_ids.append(image.getId())
    return image_ids


def get_channels_data(gateway, ctx, image_id):
    "List the channels data associated to the specified image"
    svc = gateway.getFacility(MetadataFacility)
    return svc.getChannelData(ctx, image_id)


def get_channel_wavelength(gateway, ctx, image_id, dataset_name):
    "Load the map annotations and find the channel's wavelength matching the dataset name"
    svc = gateway.getFacility(MetadataFacility)
    types = ArrayList(1)
    types.add(MapAnnotationData)
    data = ImageData(ImageI(image_id, False))
    annotations = svc.getAnnotations(ctx, data, types, None)
    # Iterate through annotation
    j = annotations.iterator()
    while j.hasNext():
        annotation = j.next()
        if annotation.getNameSpace() == NAMESPACE:
            named_values = annotation.getContent()
            i = named_values.iterator()
            while i.hasNext():
                nv = i.next()
                if nv.name == MAP_KEY:
                    channels = nv.value.split("; ")
                    for c, ch_name in enumerate(channels):
                        values = ch_name.split(":")
                        name = values[1]
                        if name in dataset_name:
                            return values[0]


def open_image_plus(HOST, USERNAME, PASSWORD, PORT, group_id, image_id):
    "Open the image using the Bio-Formats Importer"

    options = """location=[OMERO]
open=[omero:server=%s
user=%s
port=%s
pass=%s
groupID=%s
iid=%s]
 windowless=true """ % (HOST, USERNAME, PORT, PASSWORD, group_id, image_id)
    IJ.runPlugIn("loci.plugins.LociImporter", options)


def save_as_csv(rt, image_id, tmp_dir, channel_index, dataset_name):
    "Create a summary table from the original table"
    # Remove the rows not corresponding to the specified channel
    to_delete = []
    ref = "c:" + str(channel_index)
    max_bounding_box = 0
    for i in range(0, rt.size()):
        label = rt.getStringValue("Label", i)
        if ref in label:
            w = rt.getStringValue("Width", i)
            h = rt.getStringValue("Height", i)
            area = float(w)*float(h)
            max_bounding_box = max(area, max_bounding_box)

    # Rename the table so we can read the summary table
    IJ.renameResults("Results")
    rt = ResultsTable.getResultsTable()
    for i in range(0, rt.size()):
        value = rt.getStringValue("Slice", i)
        if not value.startswith(ref):
            to_delete.append(i)
    # Delete the rows we do not need
    for index, value in enumerate(to_delete):
        v = value-index
        rt.deleteRow(v)
    rt.updateResults()
    # Insert values in summary table
    for i in range(0, rt.size()):
        rt.setValue("Dataset", i, dataset_name)
        rt.setValue("Bounding_Box", i, max_bounding_box)
    # Create a tmp file and save the result
    file = tempfile.NamedTemporaryFile(mode='wb', prefix=str(image_id), suffix='.csv', dir=tmp_dir, delete=False)
    rt.updateResults()
    rt.saveAs(file.name)


def save_rois_to_omero(ctx, image_id, imp):
    # Save ROI's back to OMERO
    reader = ROIReader()
    roi_list = reader.readImageJROIFromSources(image_id, imp)
    roi_facility = gateway.getFacility(ROIFacility)
    result = roi_facility.saveROIs(ctx, image_id, exp_id, roi_list)

    roivec = []

    j = result.iterator()
    while (j.hasNext()):
        roidata = j.next()
        roi_id = roidata.getId()

        i = roidata.getIterator()
        while (i.hasNext()):
            roi = i.next()
            shape = roi[0]
            t = shape.getZ()
            z = shape.getT()
            c = shape.getC()
            shape_id = shape.getId()
            roivec.append([roi_id, shape_id, z, c, t])
    return roivec


def upload_csv_to_omero(ctx, file, type, object_id):
    "Upload the CSV file and attach it to the specified object"
    svc = gateway.getFacility(DataManagerFacility)
    file_size = os.path.getsize(file.name)
    original_file = OriginalFileI()
    original_file.setName(rstring(file.name))
    original_file.setPath(rstring(path))
    original_file.setSize(rlong(file_size))

    checksum_algorithm = ChecksumAlgorithmI()
    checksum_algorithm.setValue(rstring(ChecksumAlgorithmSHA1160.value))
    original_file.setHasher(checksum_algorithm)
    original_file.setMimetype(rstring("text/csv"))
    original_file = svc.saveAndReturnObject(ctx, original_file)
    store = gateway.getRawFileService(ctx)

    # Open file and read stream
    store.setFileId(original_file.getId().getValue())
    print original_file.getId().getValue()
    try:
        store.setFileId(original_file.getId().getValue())
        with open(file.name, 'rb') as stream:
            buf = 10000
            for pos in range(0, long(file_size), buf):
                block = None
                if file_size-pos < buf:
                    block_size = file_size-pos
                else:
                    block_size = buf
                stream.seek(pos)
                block = stream.read(block_size)
                store.write(block, pos, block_size)

        original_file = store.save()
    finally:
        store.close()

    # create the file annotation
    namespace = "training.demo"
    fa = FileAnnotationI()
    fa.setFile(original_file)
    fa.setNs(rstring(namespace))

    data_object = None
    if type == "Project":
        data_object = ProjectData(ProjectI(object_id, False))
    elif type == "Dataset":
        data_object = DatasetData(DatasetI(object_id, False))
    svc.attachAnnotation(ctx, FileAnnotationData(fa), data_object)


def save_summary_as_omero_table(ctx, file, type, object_id, delimiter):
    "Convert the CSV file into an OMERO table and attach it to the specified object"
    data = None
    with open(file.name, 'r') as f:
        headers = f.readline().strip().split(delimiter)
        # convert headers into columns
        columns = [TableDataColumn] * len(headers)
        string_indexes = []
        rows = []
        for i, c in enumerate(headers):
            if c == "Slice" or c == "Dataset":
                columns[i] = TableDataColumn(c, i, String)
                string_indexes.append(i)
            else:
                columns[i] = TableDataColumn(c, i, Double)
        # Read the rest of the file
        for line in f:
            values = line.strip().split(delimiter)
            row = []
            for i, c in enumerate(values):
                if i in string_indexes:
                    row.append(String(c))
                else:
                    row.append(Double(c))
            rows.append(row)
        data = [[Object() for x in range(len(rows))] for y in range(len(columns))]
        for r in range(0, len(rows)):
            row = rows[r]
            for i in range(0, len(row)):
                data[i][r] = row[i]
    # Create the table
    table_data = TableData(columns, data)
    table_facility = gateway.getFacility(TablesFacility)
    data_object = None
    if type == "Project":
        data_object = ProjectData(ProjectI(object_id, False))
    elif type == "Dataset":
        data_object = DatasetData(DatasetI(object_id, False))

    table_facility.addTable(ctx, data_object, "Summary_from_Fiji", table_data)


# Connect
gateway, user = connect_to_omero()
group_id = user.getGroupId()
ctx = SecurityContext(group_id)
exp = gateway.getLoggedInUser()
exp_id = exp.getId()

tmp_dir = tempfile.gettempdir()

# get all the dataset_ids in an project
datasets = get_datasets(gateway, ctx, project_id)
j = datasets.iterator()
while j.hasNext():
    d = j.next()
    name = d.getName()
    # for each dataset load the images
    # get all images_ids in the dataset
    image_ids = get_image_ids(gateway, ctx, d.getId())
    for id in image_ids:
        channel_index = 1
        # Find the index of the channel matching the dataset name as a string
        channel_wavelength = get_channel_wavelength(gateway, ctx, id, name)
        channels = get_channels_data(gateway, ctx, id)
        i = channels.iterator()
        while i.hasNext():
            channel = i.next()
            em = channel.getEmissionWavelength(None)
            if em is not None:
                v = str(int(em.getValue()))
                if channel_wavelength == v:
                    channel_index = channel.getIndex()+1
                    print "Found index: "+str(channel_index)
                    break

        open_image_plus(HOST, USERNAME, PASSWORD, PORT, group_id, id)
        imp = IJ.getImage()
        # Some analysis which creates ROI's and Results Table
        IJ.run("8-bit")
        IJ.run(imp, "Auto Threshold", "method=MaxEntropy stack")
        IJ.run(imp, "Analyze Particles...", "size=10-Infinity pixel display clear add stack summarize")
        IJ.run("Set Measurements...", "area mean standard modal min centroid center perimeter bounding feret's summarize stack display redirect=None decimal=3")

        rm = RoiManager.getInstance()
        rm.runCommand(imp, "Measure")
        rt = ResultsTable.getResultsTable()
        # Save the ROIs
        if save_data:
            roivec = save_rois_to_omero(ctx, id, imp)

        print "saving locally results for image with ID " + str(id)
        save_as_csv(rt, id, tmp_dir, channel_index, name)
        # Close the various components
        IJ.selectWindow("Results")
        IJ.run("Close")
        IJ.selectWindow("ROI Manager")
        IJ.run("Close")
        imp.changes = False     # Prevent "Save Changes?" dialog
        imp.close()

# Close the connection
gateway.disconnect()

# Aggregate the CVS files
delimiter = ","
csv_files = fnmatch.filter(os.listdir(tmp_dir), '*.csv')
file = tempfile.TemporaryFile(mode='wb', prefix='idr0021_merged_results_', suffix='.csv', dir=tmp_dir)
with open(file.name, 'wb') as master:
    master_csv = csv.writer(master)
    path = os.path.join(tmp_dir, csv_files[0])
    with open(path, 'rb') as csv_file:
        headers = csv_file.readline().strip().split(delimiter)
        master_csv.writerow(headers)
        for line in csv_file:
            master_csv.writerow(line.strip().split(delimiter))
    os.remove(path)
    for f in csv_files[1:]:
        path = os.path.join(tmp_dir, f)
        with open(path, 'rb') as csv_file:
            for line_num, line in enumerate(csv_file):
                if line_num > 0:
                    master_csv.writerow(line.strip().split(delimiter))
        os.remove(path)

# Attach the summary csv file to the Project
if save_data:
    upload_csv_to_omero(ctx, file, "Project", project_id)
    save_summary_as_omero_table(ctx, file, "Project", project_id, delimiter)
    # delete local copy of the file
    os.remove(file.name)

print "processing done"
